## Methods

Methods go here.

### Experimental

#### Primate brain organoid protocols

We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].

#### Single-cell capture of primate brain organoids

We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].

#### Long read library prep

We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].


#### Short read library prep

We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].

#### Sequencing

We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].

### Computational

#### $k$-mer comparison of orthologous genes

We used ENSEMBL version 97.
We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].

#### Extraction of putative coding reads from RNA-seq

We did things.
One sentence per line.
Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].

#### $bam2fasta$ conversion

The 10x bam file generated by the drop seq data for the different primates in this study are in the order of 6-12 GB. 
The drop seq bam files so obtained can attribute to few limitations as discussed below.
Firstly, loading them in memory all at once would require a lot of RAM depending on how your program is going to allocate memory for different data typed tags in bam file. 
Secondly, if dropseq data is not accompanied by a barcodes file to filter the .bam file on, it would mean we would have to recursively go through the alignments in the bam file and deduce alignments with higher quality and combine sequences with already exisiting barcodes. 
This would need a look up dictionary to be updated as it loops through the alignments in the bam file and would search the look up dictionary as it updates the barcodes. 
In conclusion, this is a very memory intensive process that seemed to fail on even machines with 2TB RAM.


Hence we propose a method that could work on a computer with lesser RAM and not cause computer hangups. 
We released an open source pypi package for the same - https://pypi.org/project/bam2fasta/. 
The package contains solution for the above discussed problem by sharding the bam file into chunks of bam files and stores them in the temporary folder.
The chunk size of the bam file is a parameter one can tune.
This process is done serially by iterating through the alignments in bam, read using pysam(python wrapper around samtools [@doi:10.1093/bioinformatics/btp352])
Now we employ a MapReduce [@doi:10.1145/1327452.1327492] approach to the temporary bams to obtain all the reads per cell barcode in a fasta file.
We map and distrbute the computation i.e parsing the barcode, determining the quality of the read, and if alignment is not duplicated parallely using multiple processes on the temporary shards of .bam files. 
These bam shards create temporary .fasta files that contain for each read: the cell barcode, unique molecular identifier (UMI), and the aligned sequence.
There might be a cell barcode that would be present in different chunks of these sharded .bam files.
As a result we would have multiple temporary .fasta files for the same barcodes.
We implemented a method to find the unique barcodes based on these temporary fasta file names and then assigning each of the unique barcodes all the temporary barcode .fasta files created by different .bam shards in a dictionary. 
This reduce step involves calculations and concatenation of strings of temporary fasta file names, hence its memory consumption is less. 
These temporary fasta files are then combined to one fasta file per barcode by concatenating all the sequences obtained from different fasta files. 
The concatenation of all sequences for each of the unique barcodes is also then parallelized to use multiple processes.
For each of the cell barcodes, there is an option to obtain valid cell barcodes, based on the UMI count per cell barcode.
For our datasets we have set the minimum number of UMIs per cell barcode to 1000 based on comparing the barcode count to the long read UMI count matrix. 
| Primate Species | Size of the bam file | Time taken for computation | RAM used | Processors used |
|-----------------|----------------------|----------------------------|----------|-----------------|
| Human organoid  | 12 GB                | 7 hrs                      | 16 GB    | 32              |
| Orangutan       | 9 GB                 | 4 hrs                      | 16 GB    | 32              |
| Chimp           | 9 GB                 | 4 hrs                      | 16 GB    | 32              |
This method primarily gives us time performance improvement. 
It reduces time from days or just process running out of memory to hours. Depending on the size of bam file and resources of the cluster/computer it can be further reduced. 

Prefer DOI for references, but for Biorxiv use the URL.
DOI example: [@doi:10.1038/nmeth.3830].
Biorxiv example: [@url:https://doi.org/10.1101/466201].
Multiple citations per line example: [@doi:10.1038/nmeth.3830; @url:https://doi.org/10.1101/466201].
